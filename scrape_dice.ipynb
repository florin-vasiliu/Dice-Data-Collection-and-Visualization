{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "# Headless False for displaying the browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_cards_dice(browser):\n",
    "    # get html page\n",
    "    html = browser.html\n",
    "\n",
    "    #parse request to BeautifulSoup object\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    #get page job cards\n",
    "    return soup.find_all('div', class_=\"card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in db\n",
    "class db_connection:\n",
    "    def __init__(self):\n",
    "        #connect to database\n",
    "        connection_string='mongodb://localhost:27017'\n",
    "        client = pymongo.MongoClient(connection_string)\n",
    "        #define database for storage\n",
    "        db = client.dice_db\n",
    "        #drop all stored data\n",
    "        #db.jobs.drop()\n",
    "        db.jobs\n",
    "        #define collection to store data\n",
    "        self.jobs_collection = db.jobs\n",
    "\n",
    "    # date conversion\n",
    "    def convert_date(self, date_string):\n",
    "            if date_string.find(\"hours ago\")>-1:\n",
    "                return date.today()\n",
    "            elif date_string.find(\"hour ago\")>-1:\n",
    "                return date.today()\n",
    "            elif date_string.find(\"minutes ago\")>-1:\n",
    "                return date.today()\n",
    "            elif date_string.find(\"minute ago\")>-1:\n",
    "                return date.today()\n",
    "            elif date_string.find(\"days ago\")>-1:\n",
    "                days_to_substract = re.compile(r'\\d+')\n",
    "                days_to_substract = days_to_substract.findall(date_string)[0]\n",
    "                return date.today()-timedelta(days=int(days_to_substract))\n",
    "            elif date_string.find(\"day ago\")>-1:\n",
    "                return date.today()-timedelta(days=1)\n",
    "\n",
    "    def scrape_job_dice(self, job_card):\n",
    "        #initiate fields\n",
    "        job_title = \"\"\n",
    "        job_company = \"\"\n",
    "        job_salary = \"\"\n",
    "        job_location = \"\"\n",
    "        job_date = \"\"\n",
    "        job_description = \"\"\n",
    "\n",
    "        job_title = job_card.find_all(class_=\"card-title-link\")[0].text\n",
    "        job_company = job_card.find_all(class_=\"card-company\")[0].a.text\n",
    "\n",
    "        # get location\n",
    "        job_location = job_card.find_all(id=\"searchResultLocation\")[0].text\n",
    "\n",
    "        # job date\n",
    "        job_date = job_card.find_all(class_=\"posted-date\")[0].text\n",
    "        job_date = str(self.convert_date(job_date))\n",
    "\n",
    "        # check if record is in database before scraping description\n",
    "        field_to_check = self.jobs_collection.find_one({\"$and\":[\n",
    "            {\"job_title\":job_title},\n",
    "            {\"job_company\": job_company},\n",
    "            {\"job_location\": job_location},\n",
    "                                                 ]})\n",
    "        if field_to_check is not None:\n",
    "            if field_to_check[\"job_date\"] != job_date:\n",
    "                self.jobs_collection.update_one({\n",
    "                    \"job_title\":job_title,\n",
    "                    \"job_company\": job_company,\n",
    "                    \"job_location\": job_location},\n",
    "                    {\"$set\":{\"job_date\":job_date}}\n",
    "                )\n",
    "                print(\"\"\"Job already found in database and the date was updated\n",
    "                \n",
    "                \"\"\")\n",
    "                return None\n",
    "            else:\n",
    "                print(\"\"\"Job already found in database\n",
    "                \n",
    "                \"\"\")\n",
    "                return None\n",
    "\n",
    "        # get full job descr html\n",
    "        job_descr_link = job_card.find_all(class_=\"card-title-link\")[0].get('href')\n",
    "        job_descr_html = requests.get(job_descr_link)\n",
    "        soup = bs(job_descr_html.text, 'html.parser') \n",
    "\n",
    "        #check if salary is present or not\n",
    "        try: \n",
    "            job_salary = soup.find_all(class_=\"mL20\")[0].text\n",
    "        except: \n",
    "            job_salary = ''\n",
    "            \n",
    "        #check if job type is present or not\n",
    "        try: \n",
    "            job_type = soup.find_all(\"input\",{\"id\":\"empTypeSSDL\"})[0][\"value\"]\n",
    "        except: \n",
    "            job_type = ''\n",
    "\n",
    "        # job description\n",
    "        try:\n",
    "            job_description = soup.find_all(id=\"jobdescSec\")[0].get_text()\n",
    "        except:\n",
    "            job_description = ''\n",
    "\n",
    "\n",
    "\n",
    "        #print all found details\n",
    "\n",
    "        print(f\"\"\"Inserted into database:\n",
    "            job_title: {job_title},\n",
    "            job_company: {job_company},\n",
    "            job_salary: {job_salary},\n",
    "            job_location: {job_location},\n",
    "            job_date: {job_date},\n",
    "            job_type: {job_type},\n",
    "            job_description: {job_description[:30]}\n",
    "            \n",
    "            \"\"\")\n",
    "\n",
    "        return [job_title, job_company, job_salary, job_location, job_date, job_type, job_description]\n",
    "\n",
    "    def store_job(self, title, company, salary, location, date, job_type, description):\n",
    "        self.jobs_collection.insert_one({ \\\n",
    "        \"job_title\": title, \\\n",
    "        \"job_company\": company, \\\n",
    "        \"job_salary\": salary, \\\n",
    "        \"job_location\": location, \\\n",
    "        \"job_date\": date, \\\n",
    "        \"job_type\": job_type, \\\n",
    "        \"job_description\": description \\\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering only fultime jobs\n",
    "url = \"https://www.dice.com/jobs?location=USA&latitude=37.09024&longitude=-95.712891&countryCode=US&locationPrecision=Country&radius=30&radiusUnit=mi&page=1&pageSize=100&filters.employmentType=FULLTIME&language=en\"\n",
    "browser.visit(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate database session\n",
    "session = db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "\n",
    "counter = 0\n",
    "time_start = time.time()\n",
    "while page <= 350:\n",
    "    time.sleep(10)\n",
    "    # Scrape and store in DB\n",
    "    cards = scrape_job_cards_dice(browser)\n",
    "    for card in cards:\n",
    "        # Scrape result will be None if card is already in database\n",
    "        scrape_result = session.scrape_job_dice(card)\n",
    "        if scrape_result is not None:\n",
    "            session.store_job(*scrape_result)\n",
    "            counter+=1\n",
    "\n",
    "        time_elapsed = time.time() - time_start\n",
    "        print(f\"Page: {page}\")\n",
    "        print(f\"New Jobs Scraped: {counter}\")\n",
    "        print(f\"Time Elapsed[min]: {time_elapsed/60}\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Navigate to next page\n",
    "    browser.click_link_by_partial_text('Â»')\n",
    "    page+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
