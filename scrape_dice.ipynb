{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import Nominatim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/favas/bin/chromedriver\n"
     ]
    }
   ],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date conversion\n",
    "def convert_date(date_string):\n",
    "        if date_string.find(\"hours ago\")>-1:\n",
    "            return date.today()\n",
    "        elif date_string.find(\"hour ago\")>-1:\n",
    "            return date.today()\n",
    "        elif date_string.find(\"minutes ago\")>-1:\n",
    "            return date.today()\n",
    "        elif date_string.find(\"minute ago\")>-1:\n",
    "            return date.today()\n",
    "        elif date_string.find(\"days ago\")>-1:\n",
    "            days_to_substract = re.compile(r'\\d+')\n",
    "            days_to_substract = days_to_substract.findall(date_string)[0]\n",
    "            return date.today()-timedelta(days=int(days_to_substract))\n",
    "        elif date_string.find(\"day ago\")>-1:\n",
    "            return date.today()-timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find coordinates for a given address\n",
    "def find_coordinates(address):     \n",
    "    # Try with address as is\n",
    "    geolocator = Nominatim(user_agent=\"DreamTeam\")\n",
    "    location = geolocator.geocode(address)\n",
    "    return {\"location_latitude\":location.latitude, \"location_latitude\":location.longitude}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scrape:\n",
    "    def __init__(self):\n",
    "        executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "        # Headless False for displaying the browser\n",
    "        self.browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    def visit_page(self, location=\"USA\", page_size=100, employment_type = \"FULLTIME\"):\n",
    "        # filtering only fultime jobs\n",
    "        url = f\"https://www.dice.com/jobs?location={location}&latitude=37.09024&longitude=-95.712891&countryCode=US&locationPrecision=Country&radius=30&radiusUnit=mi&page=1&pageSize={page_size}&filters.employmentType={employment_type}&language=en\"\n",
    "        self.browser.visit(url) \n",
    "        \n",
    "    def scrape_job_cards_dice(self):\n",
    "        # get html page\n",
    "        html = self.browser.html\n",
    "\n",
    "        #parse request to BeautifulSoup object\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        #get page job cards and create generator for each card\n",
    "        cards = soup.find_all('div', class_=\"card\")\n",
    "        for card in cards:\n",
    "            #initiate fields\n",
    "            job_title = \"\"\n",
    "            job_company = \"\"\n",
    "            job_location = \"\"\n",
    "            job_date = \"\"\n",
    "            job_descr_link = \"\"\n",
    "            \n",
    "            #get fields from card\n",
    "            job_title = card.find_all(class_=\"card-title-link\")[0].text\n",
    "            job_company = card.find_all(class_=\"card-company\")[0].a.text\n",
    "            job_location = card.find_all(id=\"searchResultLocation\")[0].text\n",
    "            job_date = card.find_all(class_=\"posted-date\")[0].text\n",
    "            job_date = str(convert_date(job_date))\n",
    "            job_descr_link = card.find_all(class_=\"card-title-link\")[0].get('href')\n",
    "            \n",
    "            #yield the results\n",
    "            yield {\"job_title\":job_title, \"job_company\":job_company, \\\n",
    "                   \"job_location\":job_location, \"job_date\":job_date, \"job_descr_link\":job_descr_link}\n",
    "            \n",
    "    def scrape_job_dice(self, job_descr_link):\n",
    "\n",
    "        job_descr_html = requests.get(job_descr_link)\n",
    "        soup = bs(job_descr_html.text, 'html.parser') \n",
    "\n",
    "        #check if salary is present or not\n",
    "        try: \n",
    "            job_salary = soup.find_all(class_=\"mL20\")[0].text\n",
    "        except: \n",
    "            job_salary = ''\n",
    "            \n",
    "        #check if job type is present or not\n",
    "        try: \n",
    "            job_type = soup.find_all(\"input\",{\"id\":\"empTypeSSDL\"})[0][\"value\"]\n",
    "        except: \n",
    "            job_type = ''\n",
    "\n",
    "        # job description\n",
    "        try:\n",
    "            job_description = soup.find_all(id=\"jobdescSec\")[0].get_text()\n",
    "        except:\n",
    "            job_description = ''\n",
    "\n",
    "        return {\"job_salary\":job_salary, \"job_type\":job_type, \"job_description\":job_description}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in db\n",
    "class db_connection:\n",
    "    def __init__(self):\n",
    "        #connect to database\n",
    "        connection_string='mongodb://localhost:27017'\n",
    "        client = pymongo.MongoClient(connection_string)\n",
    "        #define database for storage\n",
    "        db = client.dice_db\n",
    "        #drop all stored data\n",
    "        #db.jobs.drop()\n",
    "        db.jobs\n",
    "        #define collection to store data\n",
    "        self.jobs_collection = db.jobs\n",
    "        \n",
    "    def check_job_presence(self, job_title, job_company, job_location, job_date):\n",
    "        # check if record is in database before scraping description\n",
    "        field_to_check = self.jobs_collection.find_one({\"$and\":[\n",
    "            {\"job_title\":job_title},\n",
    "            {\"job_company\": job_company},\n",
    "            {\"job_location\": job_location},\n",
    "            {\"job_date\":job_date}\n",
    "                                                 ]})\n",
    "        if field_to_check is not None:\n",
    "                print(\"\"\"Job already found in database\n",
    "                \n",
    "                \"\"\")\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    def store_job(self, title, company, location, date, salary, job_type, description):\n",
    "        self.jobs_collection.insert_one({ \\\n",
    "        \"job_title\": title, \\\n",
    "        \"job_company\": company, \\\n",
    "        \"job_salary\": salary, \\\n",
    "        \"job_location\": location, \\\n",
    "        \"job_date\": date, \\\n",
    "        \"job_type\": job_type, \\\n",
    "        \"job_description\": description \\\n",
    "        })\n",
    "        \n",
    "        print(f\"\"\"Inserted into database:\n",
    "            job_title: {title},\n",
    "            job_company: {company},\n",
    "            job_salary: {salary},\n",
    "            job_location: {location},\n",
    "            job_date: {date},\n",
    "            job_type: {job_type},\n",
    "            job_description: {description[:30]}\n",
    "            \n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate database session and browser\n",
    "session = db_connection()\n",
    "browser = scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sleep_for_page_change = 10\n",
    "time_to_sleep_for_scraping_job_descr = 1\n",
    "browser.visit_page()\n",
    "time.sleep(time_to_sleep_for_page_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job already found in database\n",
      "                \n",
      "                \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time_to_sleep_for_scraping_job_descr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0daa1d36ed9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# scrape job description and wait\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mjob_descr_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_job_dice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcard_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"job_descr_link\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_to_sleep_for_scraping_job_descr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# store data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_to_sleep_for_scraping_job_descr' is not defined"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "\n",
    "job_inserted_counter = 0\n",
    "time_start = time.time()\n",
    "while page <= 350:\n",
    "    # Scrape and store in DB\n",
    "    for card_data in browser.scrape_job_cards_dice():\n",
    "        # if card data is in database then scrape job description and store data\n",
    "        if not session.check_job_presence( \\\n",
    "            card_data[\"job_title\"], \\\n",
    "            card_data[\"job_company\"], \\\n",
    "            card_data[\"job_location\"], \\\n",
    "            card_data[\"job_date\"], \\\n",
    "                                        ):\n",
    "            # scrape job description and wait\n",
    "            job_descr_dict = browser.scrape_job_dice(card_data[\"job_descr_link\"])\n",
    "            time.sleep(time_to_sleep_for_scraping_job_descr)\n",
    "            \n",
    "            # store data\n",
    "            session.store_job(\n",
    "                card_data[\"job_title\"], \\\n",
    "                card_data[\"job_company\"], \\\n",
    "                card_data[\"job_location\"], \\\n",
    "                card_data[\"job_date\"], \\\n",
    "                job_descr_dict[\"job_salary\"], \\\n",
    "                job_descr_dict[\"job_type\"], \\\n",
    "                job_descr_dict[\"job_description\"], \\\n",
    "                \n",
    "            )\n",
    "            job_inserted_counter+=1\n",
    "            \n",
    "        time_elapsed = time.time() - time_start\n",
    "        print(f\"Page: {page}\")\n",
    "        print(f\"New Jobs Scraped: {job_inserted_counter}\")\n",
    "        print(f\"Time Elapsed[min]: {time_elapsed/60}\")\n",
    "    \n",
    "    # Navigate to next page\n",
    "    browser.click_link_by_partial_text('»')\n",
    "    time.sleep(time_to_sleep_for_page_change)\n",
    "    page+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
